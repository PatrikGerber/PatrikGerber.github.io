<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">





  <link rel="shortcut icon" href="https://patrikgerber.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://patrikgerber.github.io/assets/css/main.css">
  <link rel="canonical" href="https://patrikgerber.github.io/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">



    <!-- <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="https://patrikgerber.github.io/">About</a>


        <a class="page-link" href="https://patrikgerber.github.io/research/">Research</a><a class="page-link" href="https://patrikgerber.github.io/teaching/">Teaching</a>
        
        <a class="page-link" href="https://patrikgerber.github.io/assets/pdf/CV.pdf">vitae</a>

      </div>
    </nav>  -->

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Patrik R√≥bert Gerber</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Patrik Robert Gerber clearfix">

  <div class="profile col one right">

      <img class="one" src="https://patrikgerber.github.io/assets/img/prof_pic.jpg">


      <div class="address">
        <!-- <p align='left'>Office 2-333A</p> <p align='left'>182 Memorial Dr</p> <p align='left'>Cambridge, MA 02142</p> -->

      </div>

  </div>

<h4><u>About</u></h4>
<p>I completed my PhD in Mathematics and Statistics at MIT under the supervision of <a href="https://math.mit.edu/~rigollet/">Philippe Rigollet</a>, my <a href="https://dspace.mit.edu/handle/1721.1/155358">thesis</a> is titled "Likelihood-Free Hypothesis Testing and Applications of the Energy Distance". Prior to MIT I obtained a Master‚Äôs in Mathematics and Statistics from Corpus Christi College at the University of Oxford. I am currently working as a quant at Citadel Securities in NYC. </p>

<p>In my free time I enjoy <a href="https://www.youtube.com/user/GerberPatrik/videos">playing piano</a>.</p>

  </article>

  <header class="post-header">
    <!-- <h1 class="post-title">Research</h1> -->
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Research clearfix">
    <!-- <div>

</div> -->
<h4><u>Publications</u></h4>

<div>
  <ol class="bibliography">
    <li>
    <div id="perceptron">

        <span class="title">Density estimation using the perceptron</span>
        <span class="author"><em>Patrik R√≥bert Gerber</em>, Tianze Jiang, Yury Polyanskiy and Rui Sun</span>

        <span class="periodical"><em>arxiv:2312.17701</em> (2023)
        </span>


      <span class="links">
        [<a href="#perceptron_abstract" class="abstract">abstract</a>]
        [<a href="https://arxiv.org/abs/2312.17701" target="_blank" rel="noopener noreferrer">arXiv</a>]
      </span>

      <!-- Hidden abstract block -->

      <span id="perceptron_abstract" class="abstract hidden">
        <p>We propose a new density estimation algorithm. Given n i.i.d. samples from a distribution belonging to a class
of densities on ‚Ñù<sup>d</sup>, our estimator outputs any density in the class whose "perceptron
discrepancy" with the empirical distribution is at most O(‚àöd/‚àön).
The perceptron discrepancy between two distributions is defined as the largest
difference in mass that they place on any halfspace of ‚Ñù<sup>d</sup>. It is shown that
this estimator achieves expected total variation distance to the truth that is almost
minimax optimal over the class of densities with bounded Sobolev norm and Gaussian
mixtures. This suggests that regularity of the prior distribution could be an
explanation for the efficiency of the ubiquitous step in machine learning that replaces optimization over large function spaces with simpler parametric
classes (e.g. in the discriminators of GANs).
</p>
<p>
We generalize the above to show that replacing the "perceptron discrepancy" with
the generalized energy distance of Sz√©keley-Rizzo further improves
total variation loss. The generalized energy distance between empirical
distributions is easily computable and
differentiable, thus making it especially useful for fitting generative models.
To the best of our knowledge, it is the first example of a distance with such
properties for which there are minimax statistical guarantees.

</p>
      </span>

    </div>
    </li>

    <li>
    <div id="gerberpoly">

        <span class="title">Likelihood-free hypothesis testing</span>
        <span class="author"><em>Patrik R√≥bert Gerber</em> and Yury Polyanskiy</span>

        <span class="periodical"><em>IEEE Transactions on Information Theory</em> (2024)
        </span>


      <span class="links">
        [<a href="#gerberpoly_abstract" class="abstract">abstract</a>]
        [<a href="https://arxiv.org/abs/2211.01126" target="_blank" rel="noopener noreferrer">arXiv</a>]
      </span>

      <!-- Hidden abstract block -->

      <span id="gerberpoly_abstract" class="abstract hidden">
        <p>Consider the problem of testing Z ~ ‚Ñô·µê vs Z ~ ‚Ñö·µê from m samples. Generally, to achieve a small error rate it is necessary and sufficient to have m‚âç1/œµ¬≤, where œµ measures the separation between ‚Ñô and ‚Ñö in total variation (ùñ≥ùñµ). Achieving this, however, requires complete knowledge of the distributions ‚Ñô and ‚Ñö and can be done, for example, using the Neyman-Pearson test. In this paper we consider a variation of the problem, which we call likelihood-free (or simulation-based) hypothesis testing, where access to ‚Ñô and ‚Ñö (which are a priori only known to belong to a large non-parametric family P) is given through n iid samples from each. We demostrate existence of a fundamental trade-off between n and m given by nm ‚âç n¬≤_ùñ¶ùóàùñ•(œµ,P), where n_ùñ¶ùóàùñ• is the minimax sample complexity of testing between the hypotheses H‚ÇÄ: ‚Ñô=‚Ñö vs H‚ÇÅ: ùñ≥ùñµ(‚Ñô,‚Ñö)‚â•œµ. We show this for three non-parametric families P: Œ≤-smooth densities over [0,1]·µà, the Gaussian sequence model over a Sobolev ellipsoid, and the collection of distributions P on a large alphabet [k] with pmfs bounded by c/k for fixed c. The test that we propose (based on the L¬≤-distance statistic of Ingster) simultaneously achieves all points on the tradeoff curve for these families. In particular, when m‚â´1/œµ¬≤ our test requires the number of simulation samples n to be orders of magnitude smaller than what is needed for density estimation with accuracy ‚âçœµ (under ùñ≥ùñµ). This demonstrates the possibility of testing without fully estimating the distributions.</p>
      </span>

    </div>
    </li>

    <li>
    <div id="robert2023kernel">

        <span class="title">Kernel-Based Tests for Likelihood-Free Hypothesis Testing</span>
        <span class="author"><em>Patrik R√≥bert Gerber</em>, Tianze Jiang, Yury Polyanskiy and Rui Sun</span>

        <span class="periodical"><em>NeurIPS</em> (2023)
        </span>


      <span class="links">
        [<a href="#robert2023kernel_abstract" class="abstract">abstract</a>]
        [<a href="https://arxiv.org/abs/2308.09043" target="_blank" rel="noopener noreferrer">arXiv</a>]
      </span>

      <!-- Hidden abstract block -->

      <span id="robert2023kernel_abstract" class="abstract hidden">
        <p>Given n observations from two balanced classes, consider the task of labeling an additional m inputs that are known to all belong to <i>one</i> of the two classes. Special cases of this problem are well-known: with complete knowledge of class distributions (n=‚àû) the problem is solved optimally by the likelihood-ratio test; when m=1 it corresponds to binary classification; and when m‚âàn it is equivalent to two-sample testing. The intermediate settings occur in the field of likelihood-free inference, where labeled samples are obtained by running forward simulations and the unlabeled sample is collected experimentally. In recent work it was discovered that there is a fundamental trade-off between m and n: increasing the data sample m reduces the amount n of training/simulation data needed. In this work we (a) introduce a generalization where unlabeled samples come from a mixture of the two classes - a case often encountered in practice; (b) study the minimax sample complexity for non-parametric classes of densities under <i>maximum mean discrepancy</i> (MMD) separation; and (c) investigate the empirical performance of kernels parameterized by neural networks on two tasks: detection of the Higgs boson and detection of planted DDPM generated images amidst CIFAR-10 images. For both problems we confirm the existence of the theoretically predicted asymmetric m vs n trade-off.</p>
      </span>

    </div>
    </li>


    <li>
    <div id="gerberhanpoly2023">

        <span class="title">Minimax optimal testing by classification</span>
        <span class="author"><em>Patrik R√≥bert Gerber</em>, Yanjun Han and Yury Polyanskiy</span>

        <span class="periodical"><em>COLT</em> (2023)
        </span>


      <span class="links">
        [<a href="#gerberhanpoly2023_abstract" class="abstract">abstract</a>]
        [<a href="https://arxiv.org/abs/2306.11085" target="_blank" rel="noopener noreferrer">arXiv</a>]
      </span>

      <!-- Hidden abstract block -->

      <span id="gerberhanpoly2023_abstract" class="abstract hidden">
        <p>This paper considers an ML inspired approach to hypothesis testing known as classifier/classification-accuracy testing (ùñ¢ùñ†ùñ≥). In ùñ¢ùñ†ùñ≥, one first trains a classifier by feeding it labeled synthetic samples generated by the null and alternative distributions, which is then used to predict labels of the actual data samples. This method is widely used in practice when the null and alternative are only specified via simulators (as in many scientific experiments). 
We study goodness-of-fit, two-sample (ùñ≥ùñ≤) and likelihood-free hypothesis testing (ùñ´ùñ•ùñßùñ≥), and show that ùñ¢ùñ†ùñ≥ achieves (near-)minimax optimal sample complexity in both the dependence on the total-variation (ùñ≥ùñµ) separation œµ and the probability of error Œ¥ in a variety of non-parametric settings, including discrete distributions, d-dimensional distributions with a smooth density, and the Gaussian sequence model. In particular, we close the high probability sample complexity of ùñ´ùñ•ùñßùñ≥ for each class. As another highlight, we recover the minimax optimal complexity of ùñ≥ùñ≤ over discrete distributions, which was recently established by Diakonikolas et al. (2021). The corresponding ùñ¢ùñ†ùñ≥ simply compares empirical frequencies in the first half of the data, and rejects the null when the classification accuracy on the second half is better than random.</p>
      </span>

    </div>
    </li>


    


<li>
<div id="chewi2022fisher">

    <span class="title">Fisher information lower bounds for sampling</span>
    <span class="author">Sinho Chewi, <em>Patrik R√≥bert Gerber</em>, Holden Lee and Chen Lu</span>

    <span class="periodical"><em>ALT</em> (2022)
    </span>


  <span class="links">
    [<a href="#chewi2022fisher_abstract" class="abstract">abstract</a>]
    [<a href="https://arxiv.org/abs/2210.02482" target="_blank" rel="noopener noreferrer">arXiv</a>]
  </span>

  <!-- Hidden abstract block -->

  <span id="chewi2022fisher_abstract" class="abstract hidden">
    <p>We prove two lower bounds for the complexity of non-log-concave sampling within the framework of Balasubramanian et al. (2022), who introduced the use of Fisher information (FI) bounds as a notion of approximate first-order stationarity in sampling. Our first lower bound shows that averaged LMC is optimal for the regime of large FI by reducing the problem of finding stationary points in non-convex optimization to sampling. Our second lower bound shows that in the regime of small FI, obtaining a FI of at most Œµ¬≤ from the target distribution requires poly(1/Œµ) queries, which is surprising as it rules out the existence of high-accuracy algorithms (e.g., algorithms using Metropolis-Hastings filters) in this context.</p>
  </span>

</div>
</li>


<li>
<div id="chewi2021query">

    <span class="title">The query complexity of sampling from strongly log-concave distributions in one dimension</span>
    <span class="author">Sinho Chewi, <em>Patrik R√≥bert Gerber</em>, Chen Lu, Thibaut Le Gouic and Philippe Rigollet</span>

    <span class="periodical"><em>COLT</em> (2022)
    </span>


  <span class="links">
    [<a href="#chewi2021query_abstract" class="abstract">abstract</a>]
    [<a href="http://arxiv.org/abs/2105.14163" target="_blank" rel="noopener noreferrer">arXiv</a>]
  </span>

  <!-- Hidden abstract block -->

  <span id="chewi2021query_abstract" class="abstract hidden">
    <p>We establish the first tight lower bound of Œ©(loglogŒ∫) on the query complexity of sampling from the class of strongly log-concave and log-smooth distributions with condition number Œ∫¬† in one dimension. Whereas existing guarantees for MCMC-based algorithms scale polynomially in Œ∫, we introduce a novel algorithm based on rejection sampling that closes this doubly exponential gap.</p>
  </span>

</div>
</li>


<li>
<div id="chewi2021rejection">

    <span class="title">Rejection sampling from shape-constrained distributions in sublinear time</span>
    <span class="author">Sinho Chewi, <em>Patrik R√≥bert Gerber</em>, Chen Lu, Thibaut Le Gouic and Philippe Rigollet</span>

    <span class="periodical"><em>AISTATS</em> (2022)</span>


  <span class="links">
    [<a href="#chewi2021rejection_abstract" class="abstract">abstract</a>]
    [<a href="http://arxiv.org/abs/2105.14166" target="_blank" rel="noopener noreferrer">arXiv</a>]
  </span>

  <!-- Hidden abstract block -->
  <span id="chewi2021rejection_abstract" class="abstract hidden">
    <p>We consider the task of generating exact samples from a target distribution, known up to normalization, over a finite alphabet. The classical algorithm for this task is rejection sampling, and although it has been used in practice for decades, there is surprisingly little study of its fundamental limitations. In this work, we study the query complexity of rejection sampling in a minimax framework for various classes of discrete distributions. Our results provide new algorithms for sampling whose complexity scales sublinearly with the alphabet size. When applied to adversarial bandits, we show that a slight modification of the Exp3 algorithm reduces the per-iteration complexity from O(K) to O(log¬≤K), where K is the number of arms.</p>
  </span>

</div>
</li>


<li>
<div id="chewi2021gaussian">
    <span class="title">Gaussian discrepancy: a probabilistic relaxation of vector balancing</span>
    <span class="author">Sinho Chewi, <em>Patrik R√≥bert Gerber</em>, Philippe Rigollet and Paxton Turner</span>

    <span class="periodical"> <em>Discrete Applied Mathematics</em> (2022)</span>


  <span class="links">
    [<a href="#chewi2021gaussian_abstract" class="abstract">abstract</a>]
    [<a href="http://arxiv.org/abs/2109.08280" target="_blank" rel="noopener noreferrer">arXiv</a>]
  </span>

  <!-- Hidden abstract block -->

  <span id="chewi2021gaussian_abstract" class="abstract hidden">
    <p>We introduce a novel relaxation of combinatorial discrepancy called Gaussian discrepancy, whereby binary signings are replaced with correlated standard Gaussian random variables. This relaxation effectively reformulates an optimization problem over the Boolean hypercube into one over the space of correlation matrices. We show that Gaussian discrepancy is a tighter relaxation than the previously studied vector and spherical discrepancy problems, and we construct a fast online algorithm that achieves a version of the Banaszczyk bound for Gaussian discrepancy. This work also raises new questions such as the Komlo ÃÅs conjecture for Gaussian discrepancy, which may shed light on classical discrepancy problems.</p>
  </span>

</div>
</li>


<li>
<div id="altschuler2021averaging">

    <span class="title">Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent</span>
    <span class="author">Jason M Altschuler, Sinho Chewi, <em>Patrik R√≥bert Gerber</em> and Austin J Stromme </span>

    <span class="periodical"><em>NeurIPS, Spotlight</em> (2021)</span>


  <span class="links">
    [<a href="#altschuler2021averaging_abstract" class="abstract">abstract</a>]
    [<a href="http://arxiv.org/abs/2106.08502" target="_blank" rel="noopener noreferrer">arXiv</a>]
</span>

  <!-- Hidden abstract block -->

  <span id="altschuler2021averaging_abstract" class="abstract hidden">
    <p>We study first-order optimization algorithms for computing the barycenter of Gaussian distributions with respect to the optimal transport metric. Although the objective is geodesically non-convex, Riemannian GD empirically converges rapidly, in fact faster than off-the-shelf methods such as Euclidean GD and SDP solvers. This stands in stark contrast to the best-known theoretical results for Riemannian GD, which depend exponentially on the dimension. In this work, we prove new geodesic convexity results which provide stronger control of the iterates, yielding a dimension-free convergence rate. Our techniques also enable the analysis of two related notions of averaging, the entropically-regularized barycenter and the geometric median, providing the first convergence guarantees for Riemannian GD for these problems.</p>
  </span>

</div>
</li>
</ol>
</div>


<p>See also my <a href="https://scholar.google.com/citations?user=kB9AcDEAAAAJ&hl=en">google scholar profile</a>.</p>

<!-- <header class="post-header">
    <h1 class="post-title">Teaching</h1> 
    <h5 class="post-description"></h5>
  </header> -->

<h4><u>Teaching</u></h4>
<p>During my Ph.D. I served as teaching assistant for the following courses:</p>
  <article class="post-content Teaching clearfix">
    <!-- <h4 id="teaching-assistant">Teaching Assistant</h4> -->

<ul>
  <li>6.3720/6.3722 ‚Äî Introduction to Statistical Data Analysis <small>(2024 Spring)</small></li>
  <li>18.821 ‚Äî Mathematics Project Laboratory <small>(2023 Fall)</small></li>
  <li>18.650 ‚Äî Fundamentals of Statistics <small>(2022 Fall, 2023 Spring)</small>
</li>
  <li>18.656/9.521/IDS.160 ‚Äî Mathematical Statistics - A non-asymptotic approach <small>(2022 Spring)</small>
</li>
  <li>15.070J/6.265J ‚Äî Discrete Probability and Stochastic Processes <small>(2021 Spring)</small>
</li>
  <li>18.675 ‚Äî Theory of Probability <small>(2020 Fall)</small>
</li>
</ul>

<!-- <h4 id="academic-mentor-at-mathroots-2020-2022">Academic mentor at ‚àömathroots <small>(2020, 2022)</small> </h4> -->
<p>I also served as academic mentor at <a href="http://mathroots.mit.edu" target="_blank" rel="noopener noreferrer">‚àömathroots</a> <small>(2020, 2022)</small>, which is  a mathematical talent accelerator summer program for high-potential high school students from underrepresented backgrounds or underserved communities.</p>



  </article>





</div>

      </div>
    </div>

</div>



      </div>
    </div>




    <footer>

  <div class="wrapper">
    ¬© Copyright 2024 .
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.


        Last updated: August 8, 2024.


<!--<a class="icon" href="mailto:%70%72%67%65%72%62%65%72@%6D%69%74.%65%64%75"><i class="fas fa-envelope" style="font-size: 14px"></i></a>
      
    <a class="icon" href="https://scholar.google.com/citations?user=kB9AcDEAAAAJ" target="_blank" title="Google Scholar" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
    <a class="icon" href="https://github.com/PatrikGerber" target="_blank" title="GitHub" rel="noopener noreferrer"><i class="fab fa-github" style="font-size: 14px"></i></a>
    <a class="icon" href="https://www.linkedin.com/in/patrik-robert-gerber" target="_blank" title="LinkedIn" rel="noopener noreferrer"><i class="fab fa-linkedin" style="font-size: 14px"></i></a>-->
    <!--  -->
    <!--  -->
    <!--  -->
    <!--  -->
    <!--  -->
</div>
</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-.min.js"></script>

<!-- Load Common JS -->
<script src="https://patrikgerber.github.io/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://patrikgerber.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://patrikgerber.github.io/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-142179739-1', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
